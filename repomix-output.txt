This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-16T00:17:10.698Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
tests/
  test_gitlab_reviewer.py
  test_llm_client.py
  test_review_strategies.py
.dockerignore
.gitignore
.repomixignore
Dockerfile.test
gitlab_reviewer.py
llm_client.py
main.py
Makefile
requirements.txt
review_strategies.py
sequence_diagram.puml
setup.cfg

================================================================
Repository Files
================================================================

================
File: tests/test_gitlab_reviewer.py
================
import pytest
from typing import Any, Dict
from gitlab_reviewer import GitLabReviewer
from review_strategies import ReviewComment


def create_test_comment(path: str, line: int, content: str) -> ReviewComment:
    """Create a test review comment.

    Args:
        path: File path
        line: Line number
        content: Comment content
    Returns:
        ReviewComment instance
    """
    return ReviewComment(path=path, line=line, content=content)


# Test cases for merge request processing
mr_process_test_cases = [
    pytest.param(
        # Test case 1: Single strategy, single comment
        {
            "project_id": 1,
            "mr_iid": 100,
            "changes": [
                {
                    "new_path": "test.py",
                    "diff": 'print("test")',
                    "line": 1,
                }
            ],
            "strategy_results": [
                [create_test_comment("test.py", 1, "Test comment")],
            ],
        },
        id="single_strategy_single_comment",
    ),
    pytest.param(
        # Test case 2: Multiple strategies, multiple comments
        {
            "project_id": 2,
            "mr_iid": 200,
            "changes": [
                {
                    "new_path": "multiple.py",
                    "diff": "def test():\n    pass",
                    "line": 1,
                }
            ],
            "strategy_results": [
                [create_test_comment("multiple.py", 1, "AI comment")],
                [create_test_comment("multiple.py", 1, "Security comment")],
            ],
        },
        id="multiple_strategies_multiple_comments",
    ),
]


@pytest.mark.parametrize("test_case", mr_process_test_cases)
def test_process_merge_request(mocker: Any, test_case: Dict[str, Any]) -> None:
    """Test merge request processing.

    Args:
        mocker: Pytest mocker fixture
        test_case: Test case data
    """
    # Mock GitLab client
    mock_gl = mocker.Mock()
    mock_project = mocker.Mock()
    mock_mr = mocker.Mock()

    # Setup mock returns
    mock_gl.projects.get.return_value = mock_project
    mock_project.mergerequests.get.return_value = mock_mr
    mock_mr.changes.return_value = {"changes": test_case["changes"]}

    # Mock strategies
    mock_strategies = []
    for result in test_case["strategy_results"]:
        strategy = mocker.Mock()
        strategy.review_changes.return_value = result
        mock_strategies.append(strategy)

    # Create reviewer instance
    reviewer = GitLabReviewer(mock_strategies)
    reviewer.gl = mock_gl

    # Execute
    reviewer.process_merge_request(test_case["project_id"], test_case["mr_iid"])

    # Verify
    mock_gl.projects.get.assert_called_once_with(test_case["project_id"])
    mock_project.mergerequests.get.assert_called_once_with(test_case["mr_iid"])
    mock_mr.changes.assert_called_once()

    # Verify each strategy was called
    for strategy in mock_strategies:
        strategy.review_changes.assert_called_once()

    # Verify comments were submitted
    expected_comment_count = sum(
        len(results) for results in test_case["strategy_results"]
    )
    assert mock_mr.discussions.create.call_count == expected_comment_count

================
File: tests/test_llm_client.py
================
import pytest
from typing import Any, Dict, List
from llm_client import LLMClient
from review_strategies import ReviewComment


@pytest.fixture
def mock_openai(mocker: Any) -> Any:
    """Mock OpenAI API responses.

    Args:
        mocker: Pytest mocker fixture
    Returns:
        Mock object for OpenAI API
    """
    mock = mocker.patch("openai.ChatCompletion.create")
    mock.return_value.choices = [
        mocker.Mock(message=mocker.Mock(content="Test feedback"))
    ]
    return mock


def test_analyze_code(mock_openai: Any) -> None:
    """Test code analysis with mock OpenAI response.

    Args:
        mock_openai: Mock OpenAI API fixture
    """
    client = LLMClient("test-key")
    changes: List[Dict[str, Any]] = [
        {
            "new_path": "test.py",
            "diff": "print('hello')",
            "line": 1,
        }
    ]

    comments: List[ReviewComment] = client.analyze_code(changes)

    assert len(comments) == 1
    assert isinstance(comments[0], ReviewComment)
    assert comments[0].path == "test.py"
    assert comments[0].line == 1
    assert comments[0].content == "Test feedback"


def test_analyze_code_error_handling(mocker: Any) -> None:
    """Test error handling during code analysis.

    Args:
        mocker: Pytest mocker fixture
    """
    mocker.patch(
        "openai.ChatCompletion.create",
        side_effect=Exception("API Error"),
    )
    client = LLMClient("test-key")
    changes: List[Dict[str, Any]] = [{"new_path": "test.py", "diff": "code", "line": 1}]

    comments: List[ReviewComment] = client.analyze_code(changes)
    assert len(comments) == 0

================
File: tests/test_review_strategies.py
================
import pytest
from typing import Any, Dict, List
from review_strategies import ReviewComment, AIReviewStrategy, SecurityReviewStrategy


def create_test_comment(path: str, line: int, content: str) -> ReviewComment:
    """Create a test review comment.

    Args:
        path: File path
        line: Line number
        content: Comment content
    Returns:
        ReviewComment instance
    """
    return ReviewComment(path=path, line=line, content=content)


# Test cases for AI review strategy
ai_review_test_cases = [
    pytest.param(
        # Test case 1: Simple Python code
        {
            "changes": [
                {
                    "new_path": "test.py",
                    "diff": 'print("test")',
                    "line": 1,
                }
            ]
        },
        [create_test_comment("test.py", 1, "Test comment")],
        id="simple_python_code",
    ),
    pytest.param(
        # Test case 2: Multiple files
        {
            "changes": [
                {
                    "new_path": "file1.py",
                    "diff": "def test():\n    pass",
                    "line": 1,
                },
                {
                    "new_path": "file2.py",
                    "diff": "class Test:\n    pass",
                    "line": 1,
                },
            ]
        },
        [
            create_test_comment("file1.py", 1, "Comment 1"),
            create_test_comment("file2.py", 1, "Comment 2"),
        ],
        id="multiple_files",
    ),
]


@pytest.mark.parametrize("changes,expected_comments", ai_review_test_cases)
def test_ai_review_strategy(
    mocker: Any,
    changes: Dict[str, List[Dict[str, Any]]],
    expected_comments: List[ReviewComment],
) -> None:
    """Test AI review strategy.

    Args:
        mocker: Pytest mocker fixture
        changes: Test changes data
        expected_comments: Expected review comments
    """
    # Mock LLM client
    mock_llm = mocker.Mock()
    mock_llm.analyze_code.return_value = expected_comments

    # Create strategy and run review
    strategy = AIReviewStrategy(mock_llm)
    result = strategy.review_changes(changes["changes"])

    # Verify results
    assert result == expected_comments
    mock_llm.analyze_code.assert_called_once_with(changes["changes"])


# Test cases for security review strategy
security_review_test_cases = [
    pytest.param(
        # Test case 1: Code with security issues
        {
            "changes": [
                {
                    "new_path": "secure.py",
                    "diff": 'password = "secret123"',
                    "new_line": 5,
                }
            ]
        },
        [create_test_comment("secure.py", 5, "Avoid hardcoding passwords")],
        id="security_issue_found",
    ),
    pytest.param(
        # Test case 2: Code without security issues
        {
            "changes": [
                {
                    "new_path": "safe.py",
                    "diff": "print('Hello, World!')",
                    "new_line": 1,
                }
            ]
        },
        [],
        id="no_security_issues",
    ),
]


@pytest.mark.parametrize("changes,expected_comments", security_review_test_cases)
def test_security_review_strategy(
    changes: Dict[str, List[Dict[str, Any]]],
    expected_comments: List[ReviewComment],
) -> None:
    """Test security review strategy.

    Args:
        changes: Test changes data
        expected_comments: Expected review comments
    """
    strategy = SecurityReviewStrategy()
    result = strategy.review_changes(changes["changes"])
    assert result == expected_comments

================
File: .dockerignore
================
.git
.gitignore
.env
__pycache__
*.pyc
.pytest_cache
*.pyo
*.pyd
.Python
env/
venv/
.coverage
htmlcov/

================
File: .gitignore
================
.venv
.aider*
__pycache__

.env

================
File: .repomixignore
================
.venv

================
File: Dockerfile.test
================
# Use Python 3.11 slim image for a smaller footprint
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTEST_ADDOPTS="--color=yes"

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first to leverage Docker cache
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt \
    pytest-cov \
    black \
    flake8 \
    mypy

# Copy project files
COPY . .

# Create a non-root user and switch to it
RUN useradd -m testuser && \
    chown -R testuser:testuser /app
USER testuser

# Set up entry point script
COPY <<EOF /app/run_tests.sh
#!/bin/bash
set -e

echo "ðŸ” Running code formatting check with Black..."
black --check .

echo "ðŸ” Running code style check with Flake8..."
flake8 .

echo "ðŸ” Running type checking with MyPy..."
mypy .

echo "ðŸ§ª Running tests with coverage..."
pytest --cov=. --cov-report=term-missing -v tests/

EOF

# Make the script executable
USER root
RUN chmod +x /app/run_tests.sh
USER testuser

# Set the entry point
ENTRYPOINT ["/app/run_tests.sh"]

================
File: gitlab_reviewer.py
================
import os
from typing import Any, Dict, List
import gitlab
from dotenv import load_dotenv
from review_strategies import ReviewStrategy, ReviewComment


class GitLabReviewer:
    """Main class for handling GitLab merge request reviews."""

    def __init__(self, review_strategies: List[ReviewStrategy]) -> None:
        """Initialize GitLab reviewer with review strategies.

        Args:
            review_strategies: List of review strategies to apply
        """
        load_dotenv()
        self.strategies = review_strategies
        self.gl = gitlab.Gitlab(
            url=os.getenv("GITLAB_URL", ""),
            private_token=os.getenv("GITLAB_TOKEN", ""),
        )

    def process_merge_request(self, project_id: int, mr_iid: int) -> None:
        """Process a merge request and add review comments.

        Args:
            project_id: GitLab project ID
            mr_iid: Merge request internal ID
        """
        project = self.gl.projects.get(project_id)
        mr = project.mergerequests.get(mr_iid)
        changes = mr.changes()

        all_comments = []
        for strategy in self.strategies:
            comments = strategy.review_changes(changes["changes"])
            all_comments.extend(comments)

        self._add_review_comments(mr, all_comments)

    def _get_merge_request_changes(
        self, merge_request: Any
    ) -> Dict[str, List[Dict[str, Any]]]:
        """Get changes from a merge request.

        Args:
            merge_request: GitLab merge request object
        Returns:
            Dictionary containing merge request changes
        """
        changes = merge_request.changes()
        return {
            "changes": [
                {
                    "new_path": change["new_path"],
                    "diff": change["diff"],
                    "new_line": change.get("new_line", 1),
                }
                for change in changes["changes"]
            ]
        }

    def _add_review_comments(
        self, merge_request: Any, comments: List[ReviewComment]
    ) -> None:
        """Add review comments to a merge request.

        Args:
            merge_request: GitLab merge request object
            comments: List of review comments to add
        """
        for comment in comments:
            merge_request.discussions.create(
                {
                    "body": comment.content,
                    "position": {
                        "position_type": "text",
                        "new_path": comment.path,
                        "new_line": comment.line,
                    },
                }
            )

================
File: llm_client.py
================
from typing import Any, Dict, List, TypedDict
import openai
from review_strategies import ReviewComment


class ChatMessage(TypedDict):
    """Type for chat message."""

    role: str
    content: str


class LLMClient:
    """Client for interacting with OpenAI's API."""

    def __init__(self, api_key: str):
        """Initialize the LLM client with API key."""
        openai.api_key = api_key

    def analyze_code(self, code_changes: List[Dict[str, Any]]) -> List[ReviewComment]:
        """
        Analyze code changes using OpenAI's API and return review comments.
        Args:
            code_changes: List of dictionaries containing code change information
        Returns:
            List of ReviewComment objects with suggestions
        """
        messages = self._prepare_messages(code_changes)
        try:
            response = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                temperature=0.7,
                max_tokens=500,
            )
            return self._parse_response(response, code_changes)
        except Exception as e:
            print(f"Error calling OpenAI API: {str(e)}")
            return []

    def _prepare_messages(
        self, code_changes: List[Dict[str, Any]]
    ) -> List[ChatMessage]:
        """Prepare messages for the OpenAI API."""
        system_msg: ChatMessage = {
            "role": "system",
            "content": "You are a helpful code reviewer. Provide concise feedback.",
        }
        user_msgs: List[ChatMessage] = []
        for change in code_changes:
            path = change["new_path"]
            diff = change["diff"]
            msg: ChatMessage = {
                "role": "user",
                "content": f"Review this code change in {path}:\n{diff}",
            }
            user_msgs.append(msg)
        return [system_msg] + user_msgs

    def _parse_response(
        self, response: Any, code_changes: List[Dict[str, Any]]
    ) -> List[ReviewComment]:
        """Parse OpenAI API response into ReviewComment objects."""
        comments: List[ReviewComment] = []
        if not hasattr(response, "choices"):
            return comments

        for idx, choice in enumerate(response.choices):
            if idx < len(code_changes):
                comments.append(
                    ReviewComment(
                        path=code_changes[idx]["new_path"],
                        line=code_changes[idx]["line"],
                        content=choice.message.content.strip(),
                    )
                )
        return comments

================
File: main.py
================
import os
from llm_client import LLMClient
from gitlab_reviewer import GitLabReviewer
from review_strategies import AIReviewStrategy, SecurityReviewStrategy


def main() -> None:
    """Main entry point for the GitLab AI reviewer."""
    llm_client = LLMClient(api_key=os.getenv("OPENAI_API_KEY", ""))
    strategies = [AIReviewStrategy(llm_client), SecurityReviewStrategy()]
    reviewer = GitLabReviewer(strategies)

    project_id = os.getenv("GITLAB_PROJECT_ID")
    mr_iid = os.getenv("GITLAB_MR_IID")

    if not project_id or not mr_iid:
        print("Error: GITLAB_PROJECT_ID and GITLAB_MR_IID must be set")
        return

    reviewer.process_merge_request(int(project_id), int(mr_iid))


if __name__ == "__main__":
    main()

================
File: Makefile
================
.PHONY: test clean

# Docker image name
IMAGE_NAME := gitlab-reviewer-test

# Default target
.DEFAULT_GOAL := test

# Clean up docker images and cache
clean:
	@echo "ðŸ§¹ Cleaning up..."
	docker rmi $(IMAGE_NAME) 2>/dev/null || true
	find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
	find . -type d -name ".pytest_cache" -exec rm -rf {} + 2>/dev/null || true
	find . -type f -name "*.pyc" -delete 2>/dev/null || true
	rm -rf .coverage htmlcov/ 2>/dev/null || true

# Run tests in Docker container
test: clean
	@echo "ðŸ—ï¸  Building test container..."
	docker build -t $(IMAGE_NAME) -f Dockerfile.test .
	@echo "ðŸ§ª Running tests..."
	docker run --rm $(IMAGE_NAME)

================
File: requirements.txt
================
python-gitlab>=3.15.0
openai>=1.3.7
pydantic>=2.5.2
python-dotenv>=1.0.0
pytest>=7.4.3
pytest-mock>=3.12.0
responses>=0.24.1

================
File: review_strategies.py
================
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, List, Optional
import re


@dataclass
class ReviewComment:
    """A code review comment."""

    path: str
    line: int
    content: str
    suggestion: Optional[str] = None


class ReviewStrategy(ABC):
    """Base class for code review strategies."""

    @abstractmethod
    def review_changes(self, changes: List[Dict[str, Any]]) -> List[ReviewComment]:
        """Review code changes and return comments.

        Args:
            changes: List of code changes to review
        Returns:
            List of review comments
        """
        pass


class AIReviewStrategy(ReviewStrategy):
    """AI-powered code review strategy."""

    def __init__(self, llm_client: Any) -> None:
        """Initialize AI review strategy.

        Args:
            llm_client: LLM client for code analysis
        """
        self.llm_client = llm_client

    def review_changes(self, changes: List[Dict[str, Any]]) -> List[ReviewComment]:
        """Review code changes using AI.

        Args:
            changes: List of code changes to review
        Returns:
            List of review comments
        """
        comments = []
        for file_change in changes:
            review = self.llm_client.analyze_changes(file_change)
            comments.extend(review)
        return comments


class SecurityReviewStrategy(ReviewStrategy):
    """Security-focused code review strategy."""

    def __init__(self) -> None:
        """Initialize security review strategy."""
        self.patterns = {
            r"password\s*=": "Avoid hardcoding passwords",
            r"token\s*=": "Avoid hardcoding tokens",
            r"eval\(": "Avoid using eval() for security reasons",
            r"exec\(": "Avoid using exec() for security reasons",
        }

    def review_changes(self, changes: List[Dict[str, Any]]) -> List[ReviewComment]:
        """Review code changes for security issues.

        Args:
            changes: List of code changes to review
        Returns:
            List of review comments
        """
        comments = []
        for change in changes:
            for pattern, message in self.patterns.items():
                if re.search(pattern, change["diff"], re.IGNORECASE):
                    comments.append(
                        ReviewComment(
                            path=change["new_path"],
                            line=change.get("new_line", 1),
                            content=message,
                        )
                    )
        return comments

================
File: sequence_diagram.puml
================
@startuml GitLab AI Review System

actor Developer
participant "GitLab\nWebhook" as Webhook
participant "Main" as Main
participant "GitLabReviewer" as Reviewer
participant "ReviewStrategy" as Strategy
participant "AIReviewStrategy" as AIStrategy
participant "SecurityReviewStrategy" as SecStrategy
participant "LLMClient" as LLM
participant "GitLab API" as GitLabAPI
participant "OpenAI API" as OpenAI

Developer -> GitLabAPI: Create/Update Merge Request
GitLabAPI -> Webhook: Trigger webhook event

Webhook -> Main: Notify new/updated MR
activate Main

Main -> Reviewer: process_merge_request(project_id, mr_iid)
activate Reviewer

Reviewer -> GitLabAPI: Get MR details
GitLabAPI --> Reviewer: MR info

Reviewer -> GitLabAPI: Get changes
GitLabAPI --> Reviewer: Changes data

loop For each strategy
    alt AI Review
        Reviewer -> AIStrategy: review_changes(changes)
        activate AIStrategy
        
        AIStrategy -> LLM: analyze_changes(file_change)
        activate LLM
        
        LLM -> OpenAI: Create completion
        OpenAI --> LLM: AI analysis
        
        LLM --> AIStrategy: Review comments
        deactivate LLM
        
        AIStrategy --> Reviewer: AI review comments
        deactivate AIStrategy
        
    else Security Review
        Reviewer -> SecStrategy: review_changes(changes)
        activate SecStrategy
        SecStrategy --> Reviewer: Security review comments
        deactivate SecStrategy
    end
end

Reviewer -> GitLabAPI: Submit review comments
GitLabAPI --> Reviewer: Confirmation

Reviewer --> Main: Review complete
deactivate Reviewer

Main --> Webhook: Process complete
deactivate Main

GitLabAPI -> Developer: Notify review complete

@enduml

================
File: setup.cfg
================
[flake8]
max-line-length = 88
extend-ignore = E203
exclude = .git,__pycache__,build,dist

[mypy]
python_version = 3.11
warn_return_any = True
warn_unused_configs = True
disallow_untyped_defs = True
disallow_incomplete_defs = True
check_untyped_defs = True
disallow_untyped_decorators = True
no_implicit_optional = True
warn_redundant_casts = True
warn_unused_ignores = True
warn_no_return = True
warn_unreachable = True

[mypy-pytest.*]
ignore_missing_imports = True
